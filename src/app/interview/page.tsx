// src/app/interview/page.tsx
"use client";

import { Suspense, useEffect, useRef, useState } from "react";
import { useAppDispatch, useAppSelector } from "@/hooks/storeHook";
import { submitAnswerAndMaybeEnd } from "@/store/interview/interviewSlice";
import RecordingControls from "@/components/interview/RecordingControls";
import QuestionDisplay from "@/components/interview/QuestionDisplay";
import UserVideo from "@/components/interview/UserVideo";
import InterviewerView from "@/components/interview/InterviewerView";
import { useRouter } from "next/navigation";
import api from "@/lib/axiosInstance";

// ğŸ”µ ì¶”ê°€: TTS
import TtsComponent from "@/components/tts/TtsComponent";

function toErrorMessage(err: unknown): string {
  if (typeof err === "string") return err;
  if (err instanceof Error) return err.message;
  return "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.";
}

export default function InterviewPage() {
  const dispatch = useAppDispatch();
  const router = useRouter();

  const { currentQuestion, interviewId, currentSeq, isFinished } =
    useAppSelector((state) => state.interview);

  const [isClient, setIsClient] = useState(false);
  const [questionStarted, setQuestionStarted] = useState(false); // ğŸ”µ TTS ëë‚˜ê¸° ì „ê¹Œì§€ false
  const [stream, setStream] = useState<MediaStream | null>(null);
  const [previewUrl, setPreviewUrl] = useState<string | null>(null);

  const { selectedVideoDeviceId, selectedAudioDeviceId, preferredVideo } =
    useAppSelector((s) => s.media);

  const submitInProgressRef = useRef(false);
  const lastKeyRef = useRef<string>("");

  // ğŸ”µ í˜„ì¬ TTS ì¬ìƒ ì¤‘ì¸ì§€ í‘œì‹œ(ê±´ë„ˆë›°ê¸° ë²„íŠ¼ í™œì„±í™” ë“±ì— í™œìš©)
  const [isTtsPlaying, setIsTtsPlaying] = useState(false);

  useEffect(() => { setIsClient(true); }, []);

  const sendEnd = async () => {
    const res = await api.post("/api/interview/end", { interviewId, lastSeq: currentSeq });
    console.log(res);
    console.log("interviewId: ", interviewId);
    localStorage.setItem("InterviewId", interviewId!);
    return res;
  };

  useEffect(() => {
    if (!isFinished) return;
    let called = false;
    (async () => {
      if (called) return;
      called = true;
      try {
        await sendEnd();
        router.replace("/");
      } catch (e) {
        console.error(e);
      }
    })();
  }, [isFinished, interviewId, currentSeq, router]);

  // ìƒíƒœ ë””ë²„ê·¸
  useEffect(() => { if (interviewId) console.log("ğŸ§© [State] interviewId:", interviewId); }, [interviewId]);
  useEffect(() => { if (currentSeq) console.log("ğŸ§© [State] currentSeq:", currentSeq); }, [currentSeq]);
  useEffect(() => { if (currentQuestion) console.log("ğŸ§© [State] currentQuestion:", currentQuestion); }, [currentQuestion]);

  // ë¯¸ë””ì–´ ìŠ¤íŠ¸ë¦¼ ì¤€ë¹„
  useEffect(() => {
    if (!isClient) return;

    let cancelled = false;
    let local: MediaStream | null = null;

    const buildVideoConstraints = (): MediaTrackConstraints => {
      const base = {
        width: preferredVideo?.width ?? { ideal: 1280 },
        height: preferredVideo?.height ?? { ideal: 720 },
        frameRate: preferredVideo?.frameRate ?? { ideal: 30 },
      };
      return selectedVideoDeviceId
        ? { ...base, deviceId: { exact: selectedVideoDeviceId } }
        : base;
    };
    const buildAudioConstraints = (): MediaTrackConstraints => {
      const base = {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      };
      return selectedAudioDeviceId
        ? { ...base, deviceId: { exact: selectedAudioDeviceId } }
        : base;
    };
    const stopTracks = (ms?: MediaStream | null) => { ms?.getTracks().forEach((t) => t.stop()); };

    const reqKey = JSON.stringify({
      v: {
        id: selectedVideoDeviceId,
        width: preferredVideo?.width ?? { ideal: 1280 },
        height: preferredVideo?.height ?? { ideal: 720 },
        frameRate: preferredVideo?.frameRate ?? { ideal: 30 },
      },
      a: { id: selectedAudioDeviceId },
    });
    if (lastKeyRef.current === reqKey) {
      console.log("â­ï¸ [MEDIA] same constraints, skip getUserMedia");
      return;
    }
    lastKeyRef.current = reqKey;

    const conservativeFallback = {
      video: { width: { ideal: 1280 }, height: { ideal: 720 }, frameRate: { ideal: 30 } },
      audio: { echoCancellation: true, noiseSuppression: true, autoGainControl: true },
    };

    const start = async () => {
      try {
        local = await navigator.mediaDevices.getUserMedia({
          video: buildVideoConstraints(),
          audio: buildAudioConstraints(),
        });
        if (cancelled) { stopTracks(local); return; }

        setStream((prev) => { stopTracks(prev); return local!; });

        // ğŸ”µ ì—¬ê¸°ì„œ ì˜ˆì „ì—” setQuestionStarted(true) í–ˆì§€ë§Œ, ì´ì œëŠ” TTS ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°!
        setQuestionStarted(false);

        const vTrack = local.getVideoTracks()[0];
        const aTrack = local.getAudioTracks()[0];
        if (vTrack) {
          console.log("ğŸ¥ [VideoTrack] label:", vTrack.label);
          console.log("ğŸ¥ [VideoTrack] settings:", vTrack.getSettings?.());
        }
        if (aTrack) {
          console.log("ğŸ™ï¸ [AudioTrack] label:", aTrack.label);
          console.log("ğŸ™ï¸ [AudioTrack] settings:", aTrack.getSettings?.());
        }
      } catch (err: unknown) {
        console.error("âŒ ë¯¸ë””ì–´ ì¥ì¹˜ ì ‘ê·¼ ì˜¤ë¥˜:", err);
        try {
          local = await navigator.mediaDevices.getUserMedia(conservativeFallback);
          if (cancelled) { stopTracks(local); return; }
          setStream((prev) => { stopTracks(prev); return local!; });
          // ğŸ”µ ìŠ¤íŠ¸ë¦¼ì€ ì¤€ë¹„ë˜ì—ˆì§€ë§Œ, ì—­ì‹œ TTS ëë‚  ë•Œê¹Œì§€ questionStarted=false ìœ ì§€
          setQuestionStarted(false);
        } catch (e2: unknown) {
          if (e2 instanceof Error) {
            console.error("ê¸°ë³¸ ì¥ì¹˜ë„ ì‹¤íŒ¨:", e2);
            alert(`ì¹´ë©”ë¼/ë§ˆì´í¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë¥˜: ${e2.message}`);
          } else {
            console.error("ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜:", e2);
            alert("ì¹´ë©”ë¼/ë§ˆì´í¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¶Œí•œ/ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.");
          }
        }
      }
    };

    start();
    return () => {
      cancelled = true;
      setStream((prev) => { stopTracks(prev); return null; });
      stopTracks(local);
    };
  }, [isClient, selectedVideoDeviceId, selectedAudioDeviceId, preferredVideo]);

  // ì œì¶œ í•¸ë“¤ëŸ¬(ê·¸ëŒ€ë¡œ)
  const handleSubmit = async (blob: Blob) => {
    if (submitInProgressRef.current || !interviewId || !currentQuestion) {
      console.warn("â³ ì œì¶œ ì¤‘ì´ê±°ë‚˜ ì¸í„°ë·° ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ ì·¨ì†Œ");
      return;
    }
    submitInProgressRef.current = true;

    const videoURL = URL.createObjectURL(blob);
    setPreviewUrl(videoURL);

    const filename = `recorded-seq-${currentSeq}.webm`;
    const file = new File([blob], filename, { type: "video/webm" });

    const formData = new FormData();
    formData.append("file", file);
    formData.append("seq", String(currentSeq));
    formData.append("interviewId", interviewId!);
    formData.append("question", currentQuestion!);

    const t0 = performance.now();
    try {
      await dispatch(submitAnswerAndMaybeEnd(formData)).unwrap();
      const t1 = performance.now();
      console.log("â±ï¸ [Timing] upload+next(+maybe end)(ms):", Math.round(t1 - t0));

      // ğŸ”µ ë‹¤ìŒ ì§ˆë¬¸ì„ ìœ„í•´ ë‹¤ì‹œ falseë¡œ ë‘ê³ , ìƒˆ ì§ˆë¬¸ì—ì„œ TTSê°€ ëë‚˜ë©´ trueê°€ ë¨
      setQuestionStarted(false);

      // (UI íš¨ê³¼)
      // setTimeout(() => setQuestionStarted(true), 400); â† ì´ì œëŠ” TTSê°€ ëë‚˜ì•¼ trueê°€ ë˜ë¯€ë¡œ ì œê±°
    } catch (e: unknown) {
      console.error("âŒ [Dispatch Failed] ì œì¶œ/ë‹¤ìŒ ì§ˆë¬¸/ì¢…ë£Œ ì˜¤ë¥˜:", e);
      alert(toErrorMessage(e));
    } finally {
      submitInProgressRef.current = false;
    }
  };

  if (!isClient) {
    return <div className="p-8 text-center">ë©´ì ‘ í™˜ê²½ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...</div>;
  }

  return (
    <Suspense>
      <div className="p-8 space-y-4">
        <QuestionDisplay question={currentQuestion} />

        {/* ğŸ”µ ì§ˆë¬¸ì´ ë°”ë€Œë©´ ìë™ìœ¼ë¡œ ì½ê³ , ëë‚˜ë©´ ë…¹í™”/íƒ€ì´ë¨¸ ì‹œì‘ ì‹ í˜¸(questionStarted=true) */}
        <TtsComponent
          text={currentQuestion ?? ""}
          autoPlay
          onStart={() => setIsTtsPlaying(true)}
          onEnd={() => {
            setIsTtsPlaying(false);
            setQuestionStarted(true); // â† ì´ ì‹œì ì— RecordingControlsê°€ ì‹œì‘
          }}
          onError={(e) => {
            console.warn("TTS ì˜¤ë¥˜, ë°”ë¡œ ë…¹í™” ì‹œì‘ìœ¼ë¡œ í´ë°±", e);
            setIsTtsPlaying(false);
            setQuestionStarted(true);
          }}
        />

        <div className="flex gap-4">
          <div className="flex-[3]">
            <InterviewerView />
          </div>

          <div className="flex-[2] flex flex-col gap-2 items-center">
            <UserVideo stream={stream} />

            {/* ğŸ”µ questionStartedê°€ trueê°€ ë˜ëŠ” ì‹œì ì€ ì˜¤ì§ TTS onEnd */}
            <RecordingControls
              stream={stream}
              questionStarted={questionStarted}
              onAutoSubmit={handleSubmit}
              onManualSubmit={handleSubmit}
            />

            {/* ğŸ”µ TTSê°€ ë§‰íˆë©´ ì‚¬ìš©ìê°€ ìˆ˜ë™ìœ¼ë¡œ ë°”ë¡œ ì‹œì‘í•  ìˆ˜ ìˆê²Œ */}
            <button
              type="button"
              className="mt-2 px-4 py-2 rounded bg-indigo-500 text-white hover:bg-indigo-600 disabled:opacity-50"
              onClick={() => setQuestionStarted(true)}
              disabled={questionStarted || !currentQuestion}
              title="TTS ê±´ë„ˆë›°ê³  ë°”ë¡œ ë‹µë³€ ì‹œì‘"
            >
              TTS ê±´ë„ˆë›°ê³  ë°”ë¡œ ë‹µë³€ ì‹œì‘
            </button>

            {previewUrl && (
              <div className="mt-4 w-full max-w-md">
                <p className="text-sm text-gray-500 mb-1">ğŸï¸ ë…¹í™”ëœ ì˜ìƒ ë¯¸ë¦¬ë³´ê¸°</p>
                <video src={previewUrl} controls className="w-full aspect-video rounded border shadow" />
              </div>
            )}
          </div>
        </div>
      </div>
    </Suspense>
  );
}
